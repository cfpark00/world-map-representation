Under review as a conference paper at ICLR 2026
ORIGINS AND ROLES OF
WORLD REPRESENTATIONS IN NEURAL NETWORKS
Anonymous authors
Paper under double-blind review
ABSTRACT
While neural representations have been extensively studied in large practical
models, the controlled conditions that govern their emergence and their down-
stream role in model adaptation remain poorly understood. We introduce a
World–Data–Model framework that separates the underlying world (fixed city
coordinates) from data generation (seven geometric tasks) and model training,
enabling causal study of how tasks shape representations and how those represen-
tations mediate fine-tuning. We show that training on single tasks yields diver-
gent representational geometries despite an identical underlying world, whereas
multi-task training drives alignment, providing controlled evidence for the Pla-
tonic Representation Hypothesis. Building on this, we investigate adaptability by
adding 100 synthetic “Atlantis” cities and fine-tuning on subsets of tasks. Sur-
prisingly, representational divergence measured in single-task pretraining predicts
downstream failure: tasks with low representational alignment not only fail to
support cross-task generalization but actively hinder it by isolating new entities
from the shared manifold. Overall, our framework establishes a model system
for holistically studying the origins, alignment, and adaptability of neural world
representations.
1 INTRODUCTION
The nature of representations and mechanisms learned by deep neural networks—or in fact any
intelligent system—and their relation to generalization is a central topic in deep learning research
(Hubel & Wiesel, 1962; Rosenblatt, 1958; Fukushima, 1980; Rumelhart et al., 1986). Recent work
has demonstrated that neural networks trained on vast amounts of data can capture diverse, disen-
tangled, and sometimes interpretable aspects of their training data, or even of the world underlying
the data (Bengio et al., 2014). These rich representations are generally thought to underlie the gen-
eralization and adaptability of neural networks to unseen, out-of-distribution scenarios.
Large language models (LLMs) (Radford et al., 2018; Devlin et al., 2018; Brown et al., 2020; Ope-
nAI, 2024), in particular, have shown striking generalization abilities that have sparked intense de-
bate about their underlying mechanisms. While there is some skepticism arguing that these models
may simply be sophisticated pattern matchers performing surface-level predictions (Bender et al.,
2021; Dziri et al., 2023; Shojaee et al., 2025), other evidence suggests that pretrained transform-
ers develop at least partial signatures of structured world models within their parameters (Li et al.,
2022; Gurnee & Tegmark, 2023; Nanda et al., 2023b; Vafa et al., 2024). Across diverse areas of deep
learning, researchers have uncovered that models represent meaningful properties of data—concepts
(Pearce et al., 2025; Higgins et al., 2017), features (Anthropic AI, 2023; Templeton et al., 2024), and
abstractions (Marks & Tegmark, 2024; Lee et al., 2025; Arditi et al., 2024)—in surprisingly inter-
pretable ways within their internal representations. These findings suggest that neural networks learn
genuine computational circuits for processing real-world concepts, rather than merely memorizing
input-output mappings.
However, major open questions remain about the origins and roles of internal representations. We
do not yet understand which properties of the world, data, and model architectures give rise to
rich representations, nor how specific properties of these representations translate to whole-network
behavior. Do representations need to be disentangled to support generalization (Locatello et al.,
2019)? Could alternative learning algorithms yield better representational structures (Kumar et al.,
1054
055
056
057

107
Under review as a conference paper at ICLR 2026
Figure 1: Overview. Our World-Data-Model framework decouples three components to study rep-
resentation learning: (1) a fixed world of city coordinates, (2) 7 geometric data generation process,
and (3) neural network models that learn from task outputs without seeing coordinates. We probe
how different tasks shape internal world representations, then test adaptability by introducing 100
synthetic “Atlantis” cities.
2025)? How do representations interact with fine-tuning? Can genuinely new ones be acquired
after pretraining? We argue that understanding these fundamental questions about the origins and
roles of neural representations is essential for the long-term goal, a model with stable and unified
representations that can be reliably updated for robust downstream adaptation.
Answering these questions is difficult in real-world training setups, where the key “knobs”—the
world, the data, and the model—are hard to control. Even the most accessible knob, the model,
becomes costly to perturb at scale, especially for LLMs. As a result, the field still lacks a holistic
framework for systematically perturbing how these different factors interact. In this work, we turn
to a small-scale synthetic setup, where the relevant factors can be precisely controlled and tested.
This work. We decouple the world from data generation (7 geometric tasks) to study how dif-
ferent tasks shape neural representations. Training small Transformers on these tasks reveals that
despite operating on the same world, different tasks create vastly different representational geome-
tries. We then reveal that multi-task training drives convergence of representations, providing con-
trolled evidence for the Platonic Representation Hypothesis (Huh et al., 2024). Lastly, we study the
role of representations for fine-tuning. When we introduce “Atlantis” cities to the world, fine-tuning
shows surprising task-dependent effects where certain tasks harm cross-task generalization. Our
contributions are as follows:
• A World-Data-Model Framework for Studying Representations. (Sec. 2) We propose treating
the world and data generation process separate, allowing systematic study of how different views
of the same world shape neural representations. Our geographic setup provides a complex yet
measurable world where representation quality can be directly assessed via coordinate probing.
• Task-Dependent Representational Geometry Despite Shared World Structure. (Sec. 3) We
show that different tasks operating on the same world produce markedly different representational
geometries.
• Multi-Task Training Drives Representational Convergence. (Sec. 3) We provide controlled
evidence for the Platonic Representation Hypothesis by showing that training on multiple tasks
leads to higher representational alignment between models which do not share any training tasks.
• Single-Task Representational Divergence Predicts Multi-Task Model Fine-Tuning Failure.
(Sec. 4) Despite joint pretraining on all tasks, we show that fine-tuning generalization is predicted
by how representations would diverge when tasks are trained in isolation. Through “Atlantis”
experiments where we add 100 new cities, we demonstrate that divergent tasks (low single-task
CKA) can actively harm generalization of fine-tuning by integrating new entities out of the pre-
trained data manifold.
2108
109
110

161
Under review as a conference paper at ICLR 2026
2 EXPERIMENTAL FRAMEWORK: GEOGRAPHIC REASONING WITH
CONTROLLABLE WORLD STRUCTURE
We begin by introducing a framework that enables systematic study of how neural networks form
world representations under different data generation processes. Our approach uses geographic
tasks where models must solve geometric problems involving city coordinates—a setup that natu-
rally separates the underlying world (city coordinates) from how data is sampled from it (geometric
tasks). This setup naturally allows changes to the underlying world (e.g., additional cities) with cor-
responding consistent updates to all dependent tasks, while providing clear metrics for measuring
representation quality through coordinate probing. Specifically, our framework provides three key
properties:
1. Consistency: All tasks are deterministically generated from the same underlying coordi-
nates, ensuring any change to city locations produces predictable updates across all geo-
metric computations.
2. Hidden State: Models never see coordinates directly, only task outputs, yet we can probe
whether they internally reconstruct the world structure.
3. Controllable Updates: We can systematically modify the world (e.g., adding new cities)
and study how models adapt their learned representations to incorporate these changes.
We settled on the setup shown in Fig. 2. We filtered the dataset of world cities by population>
100, 000, giving us 5,075 cities distributed as seen in the top of Fig. 2. We then defined 7 geometric
functions which take as input 2 or more cities and calculate a geometric value depending on the input.
Since the inputs are compositional, the data can be easily scaled to practically infinite samples.
Each task query follows a structured format where city IDs (e.g., c 1234) serve as inputs to
geometric functions, with outputs tokenized character-by-character for autoregressive prediction.
For instance, dist(c 0865,c 4879)=769 queries the distance between two cities, while
cross(c 2345,c 6789;c 0123,c 4567)=TRUE checks whether two line segments inter-
sect.
To test how models adapt to world changes, we also define a modified world with Atlantis—100
synthetic cities placed near (lon, lat) = (−35, 35) in the Atlantic Ocean. While models never
observe Atlantis during pretraining, we later use it in Sec. 4 to study whether fine-tuning on one task
with Atlantis cities enables models to integrate them into their world representations in a way that
generalizes across all tasks. This tests a critical property: can models update their internal world
model consistently when the underlying world changes?
3 TASK-DEPENDENT WORLD REPRESENTATIONS CONVERGE UNDER
MULTI-TASK LEARNING
We now investigate how neural networks develop internal world representations when trained on
different geometric tasks. We train decoder-only Transformers (Vaswani et al., 2023) on individual
tasks and task combinations, then probe their internal activations to measure whether they learn the
underlying coordinate system or merely task-specific patterns. We find that representation quality
depends critically on the task type, with different tasks inducing distinct geometries despite operat-
ing on the same world, and multi-task training driving representational convergence (see App. E for
training details).
Result 1: World representations emerge through autoregressive training. We first show ba-
sic results in Fig. 3, where we trained a model solely on the angle task. We visualize train-
ing/validation loss, PCA projections, and linearly decoded (x,y) coordinates with dots colored by
geographic regions. The model starts with nearly random representations and goes through a long
loss plateau until it finally clusters nearby cities together. After this clustering phase, the loss drops
more steeply and world representations form accordingly. It is at this moment that prediction error
on the angle task drops and the R2 for coordinate decoding from the residual stream after layer 5
becomes high. Interestingly, the coordinate decoding R2 starts to rise before we observe sudden
improvement in angle accuracy, reminiscent of Nanda et al. (2023a) who found hidden progress
measures during periods when task accuracy remains flat. Overall, we find stable formation of in-
3162
163
164

215
Under review as a conference paper at ICLR 2026
Figure 2: Experimental Setup: Seven geometric tasks. All tasks operate on the same set of
city coordinates (map shown above) but require different geometric computations: (a) Distance:
Euclidean distance, (b) Triangle Area: Area from three cities, (c) Angle: Angle at middle ver-
tex, (d) Compass: 8-way cardinal direction, (e) Inside: Point-in-polygon test, (f) Perimeter:
Polygon perimeter, (g) Crossing: Line segment intersection.
Figure 3: Emergence of World Representations. Training dynamics on the angle task reveal how
world representations form through autoregressive learning. Top panels show training/validation
loss and angle prediction accuracy over training steps. Middle panel tracks linear probe R2 for
decoding (x,y) coordinates from layer 5 activations, showing coordinate decodability emerges before
task accuracy improves. Bottom panels display PCA projections and linearly probed representations
at different training stages, progressing from random initialization through city clustering to final
world-aligned geometry. Cities are colored by geographic region throughout.
ternal world representations through pure autoregressive modeling. While the emergence of linearly
decodable coordinates might be anticipated given the geometric nature of the task, it provides a
useful validation of our framework and sets the stage for our main findings: how different data
generation processes shape these representations in fundamentally different ways.1
Result 2: Data generation process controls world representation geometry. Next, as promised,
we study how different data generation processes operating on the same underlying world shape
different model internal representations. We trained models from scratch for each of the seven tasks
shown in Fig. 2. We show four selected tasks and their representations in Fig. 4: PCA projections,
linear probe reconstructions, and rotated views.
We find that depending on the data generation process, models acquire significantly different internal
representation geometries. Some tasks form thread-like structures (distance), while others form
1We do believe linear decodability of world representation is non-trivial (albeit expected). However, this is
not the current focus of our study.
4216
217
218

268
269
Under review as a conference paper at ICLR 2026
Figure 4: World representation geometry depends on the data generation process. (a) Different
tasks create distinct geometries: distance (thread-like), angle (2D manifold), compass (frag-
mented), inside (diffuse). Row 1: PCA. Row 2: Linear probe projections. Row 3: Rotated views
showing hidden structure. (b,c) CKA matrices at layers 4 and 5. Crossing (Cr) fails to train alone.
2D manifold-like structures (angle). Compass forms less interpretable structures and inside
forms more diffuse representations. Despite these differences, we can still linearly decode (x,y)
coordinates from most tasks, as shown in the second row of Fig. 4. Some tasks (angle) form
cleaner linearly decodable world representations than others, opening the door to future study of
what drives the formation of linear representations. Note that the third dimension in the PCA plots
is the residual direction after projecting out the x,y probe directions. In the last row, we manually
rotate the 3D PCA to “flatten” the world map as much as possible. This reveals that in the non-x,y
directions, the model representations still look quite different. This reminds us that linear probing
only surfaces what we look for—akin to the parable of the blind men and the elephant, we must be
cautious not to mistake our probed coordinates for the complete representational structure.
Not shown in the figure due to space constraints, the crossing task fails to learn at all in these
single-task settings—explaining the row of zeros in the CKA plots. We speculate this connects to
known hard-to-learn dynamics and gradient plateaus in training transformers (Pezeshki et al., 2021;
Shah et al., 2020; Hoffmann et al., 2024; Bachmann & Nagarajan, 2025; Gopalani & Hu, 2025).
Intriguingly, as we will see in Result 3, this same task can be learned successfully when combined
with others in multi-task training.
To quantitatively validate our observations, we measure representational similarity between different
models’ world representations using Centered Kernel Alignment (CKA) (Kornblith et al., 2019).
Fig. 4(b,c) shows CKA between all seven models at layers 4 and 5. This reveals that the distance
task produces significantly different model representations—a result not expected intuitively. Note
again that crossing (Cr in labels) simply failed to train in isolation.
Figure 5: Multi-task pretraining drives representational convergence. (a,b) Pairwise task train-
ing creates more structured 2D manifolds than single-task models. (c) CKA matrix for all 21 pair-
wise combinations shows higher alignment. (d) Average CKA increases with task count (1→2→3),
saturating at 0.8 for layers 4-6 while layer 3 continues improving. 3D visualizations: link.
5270
271
272

322
323
Under review as a conference paper at ICLR 2026
Result 3: Task diversity aligns representations: Evidence for the Platonic Hypothesis. We
now turn to multi-task learning scenarios. While many synthetic studies focus on single tasks or
families of related tasks, real-world data more closely resembles joint learning across diverse tasks,
as explored in recent work Michaud et al. (2023); Zhang et al. (2025). Our results speak directly to
the recently proposed Platonic Representation Hypothesis (Huh et al., 2024), which observes that
neural networks trained on vast amounts of data develop aligned representations even across dif-
ferent modalities and architectures. One potential mechanism they suggest is the Multitask Scaling
Hypothesis:
“There are fewer representations that are competent for N tasks than there are for
M ≤N tasks. As we train more general models that solve more tasks at once, we
should expect fewer possible solutions.”
Our setup provides an ideal testbed for this hypothesis, with a ground-truth world model and multi-
ple tasks defined over it. We trained models on all pairwise combinations of our 7 tasks. Fig. 5(a)
shows representations when trained jointly on distance and triangle area (with single-task
models shown for comparison), while (b) shows inside and perimeter. When trained on two
tasks, models develop representational structures that better resemble curved 2D manifolds respect-
ing the world map structure. While difficult to appreciate in static 2D projections, we encourage
readers to explore our interactive 3D visualizations at this link.
To quantitatively validate these observations, we measure CKA between all pairwise-trained models
(Fig. 5(c)). Note that from 7 tasks taken 2 at a time, some models partially share one task. Even
excluding models with shared tasks, we find substantially higher CKA compared to single-task
models. In Fig. 5(d), we explicitly plot average CKA for models trained on 1, 2, and 3 tasks across
layers 3-6. For multi-task settings, we only average over models with completely disjoint task sets
(no overlap). Training on more tasks clearly leads to more aligned representations across networks.
Interestingly, CKA appears to saturate around 0.8 for 2 and 3 tasks in layers 4-6, while layer 3
continues improving with more tasks.
Overall, we find that multi-task learning leads to more aligned model internal representations. To
the best of our knowledge, this is the first experimental evidence for the Multitask Scaling Hypothe-
sis in a controlled setup. Crucially, this alignment emerges even though single-task models achieve
comparable task performance—all models reach high accuracy on their respective tasks. Since our
networks are trained to representational convergence (as seen in Fig. 3), this demonstrates that the
alignment is not simply a byproduct of optimization difficulty but rather that task diversity—not just
data quantity or performance pressure—drives aligned representation learning.
An auxiliary finding, fulfilling the promise from Result 2:
the crossing task, which was unlearnable alone, now
trains successfully when combined with other tasks. We
speculate that tasks like distance and perimeter
provide well-structured coordinate representations that
crossing can then leverage for its geometric computa-
tions—effectively creating an implicit curriculum where
easier tasks scaffold the learning of harder ones through
shared representations.
To extend these findings, we trained a model on all 7 tasks
Figure 6: 7-task model. (a) PCA re-
veals world structure. (b) Linear probe
projections. (c) Training curves for all
tasks.
simultaneously. This model successfully learns all tasks,
and its PCA projection naturally reveals the world map structure, approaching the intuitive quality of
linearly probed (x,y) coordinates without requiring any explicit coordinate supervision. This 7-task
model serves as the foundation for our fine-tuning experiments in the following section.
4 FINE-TUNING’S REPRESENTATIONAL SHIFT PREDICTS DOWNSTREAM
GENERALIZATION
In the previous section we observed how multi-task pretraining yields shared representations for
different tasks. In this section, we investigate generalization properties of fine-tuning on top of such
representations. However, unlike most fine-tuning studies which focus on changing model behavior
6324
325
326

376
377
Under review as a conference paper at ICLR 2026
in a certain way and evaluating generalization across entities, we study the inverse: fine-tuning an
entity into the model and evaluating generalization across tasks. To this end, we use the 7-task model
trained in the previous section (Fig. 6).
As mentioned in Sec. 2, we introduce 100 Atlantis cities to the world and fine-tune on data containing
Atlantis to probe for generalization. We emphasize that the introduction of Atlantis cities keeps
the original dataset fully consistent with the world. Moreover, task operations on Atlantis cities
are well-defined in the same framework. If the model learned the true data generation process
with properly factored representations, it should be able to integrate Atlantis seamlessly. If not,
we suspect either the representations are fractured (Kumar et al., 2025) or gradient descent cannot
trigger the right representational updates.
Result 1: Pretraining Phase representational
alignment predicts fine-tuning generaliza-
tion despite joint pretraining on all tasks.
We first address a fundamental question: when
fine-tuning on Atlantis cities for a single task
(e.g., distance), should we expect the model
to automatically generalize to using Atlantis for
all other tasks?
To answer this, we designed a two-stage fine-
tuning approach. First, we fine-tune on 100k
examples of a single task that include At-
lantis cities—matching the exposure Atlantis
would have received if included during pre-
training (1M examples total). Second, we add a
small elicitation set of 256 examples to ensure
Figure 7: Fine-tuning generalization and its
correlation with representational similarity. (a)
Generalization matrix showing normalized im-
provement on Atlantis queries after fine-tuning.
Rows: Task Columns: Model (b) CKA values vs.
Normalized Improvements
the model can properly handle Atlantis-specific
queries without degrading overall performance.2
The resulting generalization matrix is shown in Fig. 7. This matrix reveals rich phenomenology:
some tasks like distance show no cross-task generalization (Atlantis remains usable only for
that task), while perimeter triggers significant generalization across all tasks except triangle
area. Intriguingly, we observe an apparent inverse relationship: tasks that efficiently trigger cross-
task generalization of new entities are often those that don’t easily benefit from other tasks’ fine-
tuning—though this relationship is noisy. It is unclear if this is generally true.
Most surprisingly, we find that generalization performance correlates with the CKA values from
single-task pretraining (Result 2 of Sec. 3). This is puzzling: the CKA values come from models
trained from scratch on individual tasks, yet they predict fine-tuning behavior of a model pretrained
on all tasks jointly. If the multi-task model truly uses unified representations for cities, why would
single-task representational properties matter?
For clarity, we first define two terms: “Divergent tasks”: tasks which have low CKA compared to
others when trained in isolation (in our case the distance task), and “Hidden Spaces”: represen-
tation spaces not surfaced by PCA or probing but used by divergent tasks.
We hypothesize that even though models develop joint world representations which converge in
multi-task pretraining, gradient descent on divergent tasks might fail to act on these shared repre-
sentations during fine-tuning, instead utilizing hidden spaces that don’t propagate updates across
tasks.
Our question is then two-part:
• To what extent does this affect behavior and generalization?
• Will SGD on divergent tasks fail to merge fine-tuning introduced concepts to the original
representation manifold?
2We also mixed in 20% of the original pretraining data without Atlantis to avoid catastrophic forgetting. This
design ensures Atlantis integration happens primarily through representation learning rather than memorization.
7378
379
380

431
Under review as a conference paper at ICLR 2026
Result 2: Divergent tasks catastrophically harm generalization. To investigate how divergent
tasks affect generalization, we move from single-task to multi-task fine-tuning settings. Under a
simple additive model of fine-tuning, we would expect that fine-tuning on a concatenated dataset
{D1, D2, ..., Dn}(which do not provide conflicting supervision) would combine their individual ef-
fects. Specifically, when concatenating and shuffling all fine-tuning data to avoid sequential learning
effects like catastrophic forgetting (Kirkpatrick et al., 2017), we expect the improvement on task i
after training on tasks j and k to be:
Improvementi(j ∪k) = max(Improvementi(j), Improvementi(k)) (1)
To test this hypothesis, we fine-tuned the 7-task model on all 7
2 = 21 possible two-task com-
binations. Fig. 8(a) top shows the evaluation results across all seven tasks, while (a) bottom dis-
plays the deviation from our non-interference expectation. Strikingly, we observe “red vertical
bands”—models that not only fail to benefit from multi-task training but actually perform worse
than their best single-task component. Notably, all these degraded performance bands involve the
distance task. This confirms that divergent tasks (those with low single-task CKA) actively
harm fine-tuning generalization rather than simply failing to contribute. We next examine how this
manifests in the learned representations.
Result 3: Divergent tasks disrupt representational integration of new entities. To understand
the mechanistic basis for the performance degradation observed in Result 2, we examine how dif-
ferent task combinations affect the integration of Atlantis cities into the learned world representa-
tions. Fig. 8(b,c) compares representations from two exemplar models: one fine-tuned on angle
+ compass (non-divergent tasks) versus one fine-tuned on distance + perimeter (including
the divergent distance task).
In both PCA and linear probe visualizations, Atlantis cities integrate seamlessly into the world man-
ifold when fine-tuned on non-divergent tasks but remain segregated when divergent tasks are in-
volved. While this difference appears subtle in 2D projections, the effect is dramatic in 3D—we
strongly encourage readers to explore our interactive visualizations which clearly demonstrate the
representational segregation.
Most tellingly, when we train linear probes using only original cities (Fig. 8 c (right)) (excluding
Atlantis from probe training), the probe correctly extrapolates Atlantis locations for non-divergent
task models but places them at the origin for divergent task models. This suggests that divergent
tasks cause optimization to encode new entities in orthogonal subspaces rather than integrating them
into the existing world manifold—explaining their failure to support cross-task generalization.
We emphasize that our findings are correlational: we do not claim that interventions to increase
single-task CKA would necessarily improve fine-tuning generalization. Rather, we identify repre-
sentational divergence as a diagnostic marker for tasks that will harm multi-task fine-tuning perfor-
mance.
5 DISCUSSION
Continual world models. Recent work has focused on demonstrating that neural networks inter-
nally represent more than surface statistics and possess genuine world models. In this study, we take
a more nuanced position: these world models are often fractured and partial, as our experiments
reveal. A truly robust world model must not only represent the current state of the world but also
adapt consistently when the world changes. This adaptation is non-trivial—a single change in the
real world can require multiple cascading updates across different computational tasks. In our frame-
work, introducing Atlantis cities demands their integration across all seven geometric tasks, not just
one. The challenge becomes even more acute for tasks like counting cities within a radius, where
adding new cities changes answers even for queries that don’t directly involve the new locations.
This combinatorial explosion of interactions makes it infeasible to train on all possible ways new
data might interact with existing world knowledge. Models that rely on memorizing specific patterns
will inevitably fail when faced with systematic world changes. Instead, we argue that easily and ro-
bustly adaptable internal representations are a prerequisite for genuine world models—ones that can
update their understanding of the world without catastrophically forgetting previous knowledge or
failing to generalize the implications of new information across all relevant computations.
8432
433
434
435

484
485
Under review as a conference paper at ICLR 2026
Figure 8: Divergent tasks harm multi-task fine-tuning and disrupt representational integra-
tion. (a) Top: Performance matrix showing generalization across all 7 tasks when fine-tuning
on 21 two-task combinations. Bottom: Deviation from non-interference expectation reveals “red
vertical bands” where distance task combinations degrade performance below single-task base-
lines. (b,c) Representational analysis comparing models fine-tuned on non-divergent tasks (angle
+ compass) versus divergent task combinations (distance + perimeter). PCA projections
(left) and linear probe reconstructions (center) show Atlantis cities integrate into the world manifold
for non-divergent tasks but remain segregated in orthogonal subspaces when divergent tasks are in-
volved. (right) shows the case of training a linear probe excluding Atlantis from the training data
causing it to collapse to the center for the detached case.
Limitations. Our study operates primarily on relatively small-scale synthetic data and a single
geographic framework. While we have demonstrated that even this simplified setup can exhibit non-
trivial phenomenology—including the emergence of world representations, task-dependent factor-
ization, and off-target fine-tuning effects—it cannot fully capture the complexity of real-world data
or the scale of modern language models. Additionally, our specific choices in designing the geo-
graphic world, defining the geometric tasks, and structuring the data generation process inevitably
influence our results. However, we believe that small-scale, controllable model systems provide
crucial scientific value: they enable holistic study of the complete world→data→model pipeline,
rather than merely analyzing individual trained models post-hoc. By establishing causal relation-
ships between world structure, data generation, and representation formation in a controlled setting,
we contribute to a growing body of literature that seeks to understand not just what models learn,
but how and why they learn it. Future work should extend these findings to larger scales and more
diverse domains while maintaining the experimental control that makes mechanistic understanding
possible.
6 CONCLUSION
We introduced a controllable World–Data–Model framework to study the origins and roles of in-
ternal world representations in neural networks. Our experiments showed that different geometric
tasks induce divergent representational geometries, while multi-task training drives representational
convergence. This provides controlled evidence for the recently proposed Platonic Representation
Hypothesis (Huh et al., 2024). Crucially, we demonstrated that representational divergence in single-
task settings predicts downstream failures during fine-tuning: divergent tasks not only fail to support
integration of new entities but actively disrupt multi-task generalization. Overall, our setup marks
a step in holistically approaching the study of world representations, bridging controlled synthetic
experiments with the broader goal of building stable and adaptable neural world models.
9486
487

539
Under review as a conference paper at ICLR 2026
USE OF LARGE LANGUAGE MODELS
Large language models were used for:
• Assistance in finding related papers during literature review.
• Boilerplate code for research.
• Refining the language of the manuscript.
REPRODUCIBILITY STATEMENT
All data generation, model training and analysis were carefully tracked with configuration files to
ensure reproducibility. All random seeds for dataset generation and model training were tracked as
well (all set to 42). All code, data and analysis results will be open sources after the peer review
process. Furthermore, the authors intend to open source the entire research process including the
process on converging to the set of experiments presented in the paper.
REFERENCES
skipped
15810
811
812

863
Under review as a conference paper at ICLR 2026
APPENDIX
A RELATED WORKS
Fine-tuning. The pretraining-finetuning paradigm has become central to modern deep learning
(LeCun et al., 2015; Goodfellow et al., 2016), with remarkable success across computer vision
(Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2015; Dosovitskiy et al., 2021),
natural language processing (Howard & Ruder, 2018; Peters et al., 2018; Devlin et al., 2018),
and reinforcement learning (Team et al., 2024; Baker et al., 2022; Christiano et al., 2017; Rad-
ford et al., 2018; 2019). Despite its widespread adoption and extensive study across diverse direc-
tions—parameter efficiency (Hu et al., 2021; Lester et al., 2021), zeroth-order optimization (Malladi
et al., 2024), weight composition (Ilharco et al., 2023), and representation adaptation (Wu et al.,
2024)—fine-tuning remains surprisingly unpredictable. Models exhibit poorly understood behav-
iors such as the reversal curse (Berglund et al., 2024), out-of-context reasoning limitations (Treut-
lein et al., 2024), and off-target effects (Betley et al., 2025). Recent mechanistic studies suggest
fine-tuning may merely form a “thin wrapper” around pretrained representations rather than learn-
ing fundamentally new capabilities (Jain et al., 2023; Ward et al., 2025), while behavioral analyses
reinforce this pessimism (Yue et al., 2025; Zhao et al., 2025; Qin et al., 2025). The field is thus left
with a critical open question about whether fine-tuning can genuinely teach models new concepts
(Park et al., 2025; Zweiger et al., 2025) or is fundamentally limited to adapting existing ones.
Interpretability & Internal Representations. Understanding internal representations has been
fundamental to neuroscience long before artificial networks existed (Hubel & Wiesel, 1962), and
this focus naturally carried over to the development of artificial neural networks (Rosenblatt, 1958;
Fukushima, 1980; Rumelhart et al., 1986; Bengio et al., 2014). Recent interpretability work has
revealed that language models develop structured “world models” that encode geographic, temporal,
and relational information in their parameters (Li et al., 2022; Gurnee & Tegmark, 2023; Nanda
et al., 2023b; Vafa et al., 2024), with similar representations emerging during in-context learning
(Park et al., 2024a). This has led to the Platonic Representation Hypothesis, which posits that diverse
models trained on different modalities converge toward similar representational structures (Li et al.,
2016; Huh et al., 2024). Yet the relationship between representations and training dynamics remains
poorly understood. Only recent work has begun examining how representations emerge during
pretraining in real LLMs (Li et al., 2025; Ge et al., 2025) or how they change during fine-tuning
(Minder et al., 2025; Lee et al., 2024). The precise mechanisms determining which representations
form and how they evolve throughout both pretraining and adaptation remain open questions.
Synthetic Data. While large language models provide rich testbeds for studying neural network
behavior, their computational cost makes systematic studies of training dynamics prohibitive—one
cannot easily test hypotheses by altering pretraining pipelines. Synthetic approaches have success-
fully addressed this limitation in specific domains: understanding in-context learning (Xie et al.,
2021; Chan et al., 2022; Reddy, 2023; Ravent´ os et al., 2023; Park et al., 2024b; Wurgaft et al.,
2025), compositional generalization (Okawa et al., 2024; Park et al., 2024c), grammar/knowledge
acquisition (Allen-Zhu & Li, 2023b;a), and interpretability methods (Menon et al., 2025; Hindupur
et al., 2025). Most relevant to our work, Jain et al. (2023) used synthetic data to argue fine-tuning
creates only thin wrappers over pretrained capabilities, while Nishi et al. (2024) studied formation
and destruction of representational structure. However, existing synthetic frameworks typically de-
sign data generation processes without explicitly distinguishing between the underlying world and
how data is sampled from it. Our work introduces a framework that makes this distinction explicit,
enabling systematic study of how different views of the same world shape neural representations
and their downstream adaptability.
B 3D VISUALIZATIONS
3D visualizations are available here (Open science Framework Anonymyzed link).
16864
865
866

917
Under review as a conference paper at ICLR 2026
Figure 9: Geographic distribution of 5,175 cities used in our experiments. Cities span all continents
and provide a fixed, measurable world structure. The synthetic Atlantis region (100 cities in Atlantic
Ocean) is used for out-of-distribution testing.
C WORLD SETUP
Our experiments use a geographic world consisting of 5,075 cities extracted from the GeoN-
amesOpenDataSoft / GeoNames (2025) database with population greater than 100,000. Cities are
distributed across all continents. This choice provides natural variation in density (e.g., dense re-
gions like India versus sparse Oceania) that creates interesting computational challenges.
While we use real city coordinates, this work studies abstract geometric reasoning rather than ac-
tual geography—we simply project coordinates to Euclidean space (x, y) = (10 ∗longitude, 10 ∗
latitude) and treat all tasks as pure geometry problems.
Additionally, we introduce 100 synthetic “Atlantis” cities positioned in the Atlantic Ocean, centered
at lon -35 lat 35 and following a Gaussian distribution with standard deviation of 3 degrees. These
synthetic cities enable controlled out-of-distribution experiments, as models never observe Atlantis
during pretraining but must generalize to it during evaluation. City IDs are randomly assigned from
the range [0, 9999], creating a sparse identifier space that models must learn to map to continuous
coordinates.
It is important to note that for all tasks we study, queries that don’t explicitly involve Atlantis cities
maintain identical outputs after Atlantis is introduced—ensuring we can cleanly measure integration
of new knowledge. While our framework could be extended to study tasks where existing answers
change (e.g., counting cities within a radius would yield different results after adding Atlantis),
enabling investigation of phenomena like the reversal curse (Berglund et al., 2024), we focus here
on the simpler case of integrating new entities while preserving existing knowledge.
D TASKS AND DATASETS
We implement 7 geometric tasks that require understanding city coordinates:
• Distance (D - 1): Euclidean distance between two cities. e.g. d i s t ( c 0 8 6 5 , c 4 8
7 9 ) = 7 6 9
17918
919

971
Under review as a conference paper at ICLR 2026
• Triangle Area (T - 2): Area of triangle formed by three cities. e.g. t r i a r e a ( c 1 2
3 4 , c 5 6 7 8 , c 9 0 1 2 ) = 4 5 8 2 3 1
• Angle (A - 3): Angle at middle city of three cities (degrees). e.g. a n g l e ( c 2 3 4 5 , c
6 7 8 9 , c 0 1 2 3 ) = 9 7
• Compass (Co - 4): Direction from one city to another (8 directions). e.g. c o m p a s s ( c
1 2 3 4 , c 5 6 7 8 ) = N E
• Inside (I - 5): Whether a city lies within convex hull of others (binary). e.g. i n s i d e (
c 9 0 1 2 ; c 3 4 5 6 , c 7 8 9 0 , c 1 2 3 4 , c 5 6 7 8 ) = F A L S E
• Perimeter (P - 6): Perimeter of polygon formed by cities. e.g. p e r i m e t e r ( c 4 5 6
7 , c 8 9 0 1 , c 2 3 4 5 , c 6 7 8 9 ) = 2 8 5 6
• Crossing (Cr - 7): Whether line segments between four cities intersect (binary). e.g. c r
o s s ( c 2 3 4 5 , c 6 7 8 9 ; c 0 1 2 3 , c 4 5 6 7 ) = T R U E
Each pretraining set consists of 1M rows of data per task. For fine tuning we prepare a 100k dataset
which is required to have at least one Atlantis city in the query. We use character-level tokenization,
98 ASCII tokens, excluding space, the delimiter.
E MODEL AND TRAINING
E.1 PRETRAINING DETAILS
We train models autoregressively on character-tokenized sequences, where each task query and
answer is tokenized character-by-character (e.g., dist(c 0865,c 4879)=769 becomes d i
s t ( c 0 8 6 5 , c 4 8 7 9 ) = 7 6 9). While we observed training speedup
when masking loss computation on the prompt side (which is unpredictable), we deliberately avoid
this optimization to maintain similarity with standard autoregressive language model pretraining.
This ensures our findings about representation formation are applicable to standard LLM training
regimes. We use the Qwen-2.5 (Yang et al., 2024) architecture with a hidden size of 128, 4 attention
heads, and 6 layers. We use a batch size of 128 and train for 42M rows consistently independent
of dataset size. We use the AdamW (Loshchilov & Hutter, 2019) optimizer with learning rate 3e-4
with a linear lr scheduler and 50 warmup steps.
E.2 FINE-TUNING
We typically fine-tuned models with a learning rate of 1e−5, with the same linear learning rate
decay with warmup scheduling. We observed significant degradation in performance for both the
fine-tuned task’s and the original (non Atlantis) tasks when we used a batch size of 512.
F ANALYSIS METHODS
F.1 PROBING DETAILS
Unless otherwise mentioned we use a linear probe trained without any regularization term to extract
the optimal probes for x and y. We typical concatenate the representation of the last id token of a city,
i.e. the representation of “4” for “c 1 2 3 4”, and the representation of the following transition token,
most often a comma. For all single task models we extract the representaion as if a task was starting,
i.e. “t r i a r e a ( c 1 2 3 4 ,” is passed into the transformer and the last two tokens’ representations
are concatenated. For multi task models we use the distance task prefix, while we checked that the
prefix’s effect on the city representation is minimal. We use the same representations for PCA and
Linear probing.
Omitting irrelevant features We omit cities with ID starting with 0, 00 or 000 since the models
form a strong special representation separating cities by the number of zeros in their prefix. We
suspect this is since many tasks involves a cardinal interpretation of subsequent number tokens
for distance, angle, etc while for city ids the numbers do not contain such meaning, and cardinal
numbers are given without leading zeros, thus promoting the model to construct a feature clearly
18Under review as a conference paper at ICLR 2026
972
973
1025
distinguishing the two use cases of numbers. For fair evaluation of all cities, we consistently omit
all cities with ID starting with 0, 00 or 000 for the ease of analysis.
G CODE AND DATA AVAILABILITY
Code, data and model checkpoints will be available after the review process.
19

