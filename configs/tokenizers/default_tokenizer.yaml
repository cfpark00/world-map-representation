# WM1 tokenizer: ASCII printable characters (no space) without two-digit tokens
# This gives us flexibility for future tasks while keeping vocab size reasonable

tokenizer_type: character

output_dir: data/tokenizers/default_tokenizer

# Character set - ASCII printable minus space
charset: custom

# All printable ASCII EXCEPT space (ASCII 33-126, excluding 32)
# Space is the delimiter between tokens, NOT a token itself!
# Includes: ! " # $ % & ' ( ) * + , - . / 0-9 : ; < = > ? @ A-Z [ \ ] ^ _ ` a-z { | } ~
custom_chars: "!\"#$%&'()*+,-./:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~0123456789"

# No additional multi-character tokens (unlike RO_1 which has two-digit numbers)
# This keeps vocab size at ~98 tokens instead of ~198
additional_tokens: []

# Special tokens
special_tokens:
  bos_token: "<bos>"
  eos_token: "<eos>"
  unk_token: "<unk>"
  pad_token: "<pad>"

# Total vocab size will be:
# - 4 special tokens
# - 94 ASCII characters (all printable except space)
# = 98 tokens total