# Fine-tuning on inter-Atlantis pairs only
exp_dir: "outputs/experiments/atlantis_inter_finetune"

# Task type (distance prediction)
task_type: "distance"

# Dataset (will be created by create_atlantis_inter_dataset.py)
dataset:
  path: "outputs/datasets/atlantis_inter_4k_42"
  max_sequence_length: 32

# Tokenizer
tokenizer_path: "outputs/tokenizer/wm1_tokenizer"

# Model - loading from pre-trained distance model
model:
  vocab_size: 44  # Our custom tokenizer vocab size
  hidden_size: 128
  num_hidden_layers: 6
  num_attention_heads: 4
  intermediate_size: 512
  init_scale: 0.1  # Not used when loading from checkpoint
  # Using the final distance model checkpoint
  ckpt: "/n/home12/cfpark00/WM_1/outputs/experiments/dist_100k_1M_20epochs/checkpoints/final"

# Training - smaller learning rate for fine-tuning
training:
  batch_size: 512  # Same as original training
  eval_batch_size: 32
  num_epochs: 500  # More epochs since we have fewer samples (4628 total)
  optimizer: "adamw"
  learning_rate: 1e-4  # Lower learning rate for fine-tuning
  weight_decay: 0.01
  scheduler: "linear_with_warmup"
  warmup_steps: 20  # Fewer warmup steps
  seed: 42
  loss_mask_type: "answer_only"  # Only compute loss on distance values

# Checkpointing
checkpointing:
  save_strategy: "steps"
  save_steps: 0.05  # Save 5 times per epoch
  eval_strategy: "steps"
  eval_steps: 0.025  # Eval 10 times per epoch