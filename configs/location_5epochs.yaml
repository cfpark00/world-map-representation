# Experiment directory - all outputs go here
exp_dir: "outputs/experiments/location_5epochs"

# Dataset
dataset:
  path: "/n/home12/cfpark00/WM_1/outputs/datasets/loc_100kplus_all_42"
  max_sequence_length: 32

# Tokenizer
tokenizer_path: "outputs/tokenizer/wm1_tokenizer"

# Model (Qwen2.5-like)
model:
  vocab_size: 44  # Our custom tokenizer vocab size
  hidden_size: 64
  num_hidden_layers: 4
  num_attention_heads: 4
  intermediate_size: 256

# Training
training:
  batch_size: 512
  eval_batch_size: 128
  num_epochs: 5  # Train for 5 epochs as requested
  optimizer: "adamw"
  learning_rate: 3e-4
  weight_decay: 0.01
  warmup_steps: 50
  seed: 42
  loss_mask_type: "answer_only"  # Only compute loss on the location answer part

# Checkpointing
checkpointing:
  save_strategy: "steps"  # Can be "steps", "epoch", or "no"
  save_steps: 0.2  # Save 5 times per epoch (1/0.2 = 5)
  eval_strategy: "steps"
  eval_steps: 0.1  # Evaluate 10 times per epoch