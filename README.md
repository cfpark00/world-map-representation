# WM_1: World Model Experiments

A collection of experiments training small transformer models to learn geographic and spatial relationships.

## Project Structure

```
.
├── configs/                 # Configuration files for experiments
├── src/                     # Source code (main codebase)
│   ├── data_processing/     # Dataset creation and processing
│   ├── training/           # Model training scripts
│   ├── tokenizer/          # Tokenizer configuration
│   └── visualization/      # Plotting and analysis
├── analysis/               # Analysis scripts (to be merged into src)
├── notebooks/              # Interactive Jupyter notebooks
├── outputs/                # All generated artifacts
│   ├── datasets/           # Processed datasets
│   ├── experiments/        # Training runs and checkpoints
│   ├── figures/            # Generated visualizations
│   └── tokenizer/          # Saved tokenizer files
├── data/                   # Raw input data (not generated by us)
├── claude_notes/           # Claude Code tracking and notes
└── scratch/                # Temporary files for debugging/testing
```

### Directory Purposes

- **`configs/`**: YAML configuration files for training and experiments
- **`src/`**: Main source code for all processing, training, and analysis
- **`analysis/`**: Additional analysis scripts (will be merged into src)
- **`notebooks/`**: Interactive Jupyter notebooks for exploration
- **`outputs/`**: Everything produced by running code (datasets, models, figures, etc.)
- **`data/`**: Original input data required to begin (e.g., city coordinates)
- **`claude_notes/`**: Notes and tracking for Claude Code sessions
- **`scratch/`**: Temporary workspace for testing (pip debugging, syntax testing, etc.)

## Setup

Create and activate a Python environment:
```bash
uv venv
source .venv/bin/activate
```

Install dependencies:
```bash
uv pip install torch transformers datasets tokenizers matplotlib pandas numpy scipy tqdm pyyaml protobuf
```

## Tokenizer

The project uses a custom character-level tokenizer with 45 tokens:
- 3 special tokens: `<bos>`, `<eos>`, `<pad>`
- 5 grammar tokens: `(`, `)`, `"`, `,`, `=`, `_`
- 26 lowercase letters: a-z
- 10 digits: 0-9

Create the HuggingFace-compatible tokenizer:
```bash
python src/tokenizer/create_hf_tokenizer.py
```

Load in Python:
```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('outputs/tokenizer/wm1_tokenizer')
```

## Dataset Generation

### 1. Distance Prediction Dataset
Predicts geodesic distance between city pairs:
```bash
python src/data_processing/create_distance_dataset_hf.py 8000 1000 1000 outputs/datasets/dist_8k --seed 42
```
Format: `d(c_1234,c_5678)=2500`

### 2. Distance Threshold Dataset  
Binary classification if cities are within threshold:
```bash
python src/data_processing/create_distancethres_dataset_hf.py 8000 1000 1000 outputs/datasets/dist_thres_8k --seed 42
```
Format: `dt200(c_1234,c_5678)=0` or `=1`

### 3. Random Walk Dataset
Sequential paths through cities within distance constraints:
```bash
python src/data_processing/create_randomwalk_dataset_hf.py 1000 100 100 outputs/datasets/walk_1k --seed 42 --max-length 32 --visualize 10
```
Format: `srd_200=c_1234,c_5678,c_9012,...`

## Training

Run training with a config file:
```bash
python src/training/train.py configs/training_config.yaml
```

The training script will:
- Load dataset from the specified path
- Train a small transformer model
- Save checkpoints and visualizations to outputs/experiments/
- Evaluate on validation set during training

## Model Architecture

Default configuration (can be modified in configs):
- **Hidden size**: 64
- **Layers**: 4
- **Attention heads**: 4
- **Max sequence length**: 512
- **Positional encoding**: RoPE

## Data Format Examples

- **Distance**: `d(c_1234,c_5678)=2500` (distance in km)
- **Threshold**: `dt200(c_1234,c_5678)=1` (within 200km)
- **Random Walk**: `srd_200=c_1234,c_5678,c_9012` (sequential path)