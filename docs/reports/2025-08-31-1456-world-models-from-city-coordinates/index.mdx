import Image from 'next/image'
import cityDensityMap from './city_density_map.png'
import batchSizeComparison from './batch_size_comparison.png'
import taskExamples from './task_examples.png'
import debuggingTimeline from './debugging_timeline.png'

**TL;DR:** We're investigating when and why neural networks form modular vs fractured representations. As a testbed for this broader research program, we developed a city coordinate prediction task that offers controllable complexity, natural visualization, and rich structure. After debugging critical training issues, we found this setup produces analyzable representations perfect for studying representation formation dynamics.

## The Broader Vision: Understanding Representation Formation

Our lab is tackling a fundamental question in AI interpretability: What conditions determine whether neural networks develop clean, modular representations versus fractured, entangled ones? While existing research identifies representations after training, we want to understand the causal mechanisms that create them in the first place.

The challenge? Real-world datasets are too complex and expensive to allow controlled causality studies. We need synthetic tasks that are:
- **Controllable**: We can adjust complexity systematically
- **Rich**: Complex enough to form interesting representations
- **Analyzable**: Simple enough to understand what's happening
- **Scalable**: Can study from tiny to large models

## Why Geographic Coordinates Could Be Perfect

Here's what motivated this specific approach: When we train transformers on seemingly trivial geographic tasks, do they develop structured world models? Or are they just memorizing mappings?

City coordinates turned out to be an ideal testbed for our broader representation research because they offer:

- **Natural Hierarchy**: Cities cluster into countries, regions, continents—perfect for studying modular vs entangled representations
- **Controllable Complexity**: We can vary from 100 to 10,000+ cities, testing scaling dynamics
- **Multiple Task Types**: Distance, location, path planning—each potentially forming different representations
- **Ground Truth Visualization**: Unlike abstract synthetic data, we can literally plot representations on a map
- **Real-World Structure**: Not random patterns, but meaningful geographic/demographic correlations

## The Beautiful Bias: A Feature, Not a Bug

Geographic city data has a fascinating property: it's heavily biased. Tokyo alone has over 100 cities within 200km, while vast stretches of Siberia, the Sahara, and the Amazon have almost none. 

This bias is exactly what makes it perfect for studying representation formation:

<div className="flex justify-center my-6">
  <figure className="w-full max-w-3xl">
    <Image src={cityDensityMap} alt="Global distribution of cities showing density patterns" />
    <figcaption className="text-center text-sm text-muted-foreground mt-2">
      <strong>Figure 1:</strong> The natural clustering of cities reveals geographic, economic, and historical patterns. Dense regions like Japan and Europe contrast sharply with sparse areas like the Sahara and Pacific Ocean.
    </figcaption>
  </figure>
</div>

This non-uniform distribution creates natural challenges for representation learning:
- **Sparse vs Dense Regions**: Does the model develop different representations for crowded Europe vs empty Pacific?
- **Hierarchical Structure**: Will it discover countries and continents without being told?
- **Correlation Patterns**: Can it learn that coastal cities cluster differently than inland ones?
- **Scaling Dynamics**: How do representations change as we go from 100 to 5,000 cities?

These are exactly the kinds of questions our broader research program needs to answer about representation formation.

## The Initial Approach: Three Simple Tasks

We designed three tasks of increasing complexity:

<div className="flex justify-center my-6">
  <figure className="w-full max-w-3xl">
    <Image src={taskExamples} alt="Three task formats for city prediction" />
    <figcaption className="text-center text-sm text-muted-foreground mt-2">
      <strong>Figure 2:</strong> Three tasks designed to test different aspects of geographic understanding, from simple distance calculation to complex path generation.
    </figcaption>
  </figure>
</div>

### 1. Distance Prediction
```
Input:  dist(c_847,c_3924)=
Output: 2451
```
Given two city IDs, predict the distance between them in kilometers.

### 2. Location Prediction  
```
Input:  loc(c_847)=
Output: 4862,1787
```
Given a city ID, predict its coordinates (encoded as integers).

### 3. Random Walk Generation
```
Input:  walk_200=
Output: c_847,c_912,c_445,c_1332...
```
Generate a sequence of cities where each is within 200km of the previous one.

Simple tasks, right? The model just needs to memorize a lookup table. Or so we thought.

## The Debugging Odyssey: When Simple Tasks Aren't Simple

<div className="flex justify-center my-6">
  <figure className="w-full max-w-3xl">
    <Image src={debuggingTimeline} alt="Timeline of debugging process" />
    <figcaption className="text-center text-sm text-muted-foreground mt-2">
      <strong>Figure 3:</strong> Our debugging journey from complete failure to the embarrassingly simple solution.
    </figcaption>
  </figure>
</div>

### Round 1: The Model That Couldn't Memorize

Our first surprise: the model couldn't even memorize 1,000 city locations. After 100 epochs, it was still producing garbage outputs. This was deeply concerning—transformers are supposed to be excellent at memorization.

Initial hypotheses:
- Model too small? (No, 64 hidden dims should suffice for 1,000 items)
- Learning rate wrong? (Tried 1e-4 to 1e-2, no difference)
- Tokenization issues? (Character-level, seemed fine)
- Architecture bug? (Qwen2.5 config, standard implementation)

### Round 2: The Sanity Check Dataset

To isolate the problem, we created a "random4to8" dataset:
```
Input:  loc(c_1234)=
Output: 5678,9012
```
Completely random 4-digit inputs mapping to random 8-digit outputs. Pure memorization, no structure.

The model still failed.

This ruled out:
- Geographic complexity
- Coordinate system issues  
- Data distribution problems

Something fundamental was wrong with training.

### Round 3: The Biased Random Dataset

Next experiment: What if multiple inputs map to the same output?
```
loc(c_1234)=5678,9012
loc(c_5555)=5678,9012  # Same output!
loc(c_9999)=5678,9012  # Same output!
```

We made 50% of samples share outputs with other samples. If the model couldn't handle many-to-one mappings, this would reveal it.

Surprisingly, the model learned this fine! Many-to-one wasn't the issue.

## The Aha Moment: Batch Size vs Dataset Size

After days of debugging, we finally found it. Our configuration had:
- **Dataset size**: 1,051 samples
- **Batch size**: 512
- **Result**: Only ~2 gradient updates per epoch!

We were effectively training for only 200 steps across 100 epochs. No wonder the model couldn't memorize—it barely saw the data!

<div className="flex justify-center my-6">
  <figure className="w-full max-w-3xl">
    <Image src={batchSizeComparison} alt="Comparison of gradient updates with different batch sizes" />
    <figcaption className="text-center text-sm text-muted-foreground mt-2">
      <strong>Figure 4:</strong> The critical difference: with batch size 512, we got only 2 updates per epoch. With batch size 64, we got 16 updates—an 8x increase in actual training.
    </figcaption>
  </figure>
</div>

The fix was embarrassingly simple:
```yaml
# Before (broken)
batch_size: 512
n_epochs: 100
# Total updates: ~200

# After (working)  
batch_size: 64
n_epochs: 100
# Total updates: ~1,600
```

Eight times more gradient updates. The model immediately started learning.

## The Lesson: Steps Matter More Than Epochs

This revealed a critical insight about transformer training:

**Epochs are meaningless. Steps are everything.**

A model trained for:
- 1,000 epochs with batch_size=1024 gets 1,000 updates
- 10 epochs with batch_size=1 gets 10,000 updates

The second model sees 10x more gradient updates despite 100x fewer "epochs."

For small datasets especially, batch size becomes a critical hyperparameter—not for memory reasons, but for learning dynamics. Too large, and you simply don't train.

## The Silver Lining: Critical Insights for Representation Research

This debugging journey wasn't wasted time. It revealed crucial insights for our broader representation formation research:

1. **Gradient Update Frequency Matters**: Representation formation might depend more on update frequency than data diversity
2. **Small-Scale Experiments Are Valid**: With proper batch sizes, we can study representation dynamics even with 1,000 samples
3. **Controlled Ablations Work**: Our sanity datasets (random mappings) provide perfect controls for causal studies
4. **Many-to-One Mappings**: The model handles non-injective functions, suggesting representations can handle ambiguity

## Setting Up for Success: Our Final Configuration

After all this debugging, here's what worked:

### Model Architecture
- **Base**: Qwen2.5-style transformer
- **Size**: 64 hidden dims, 4 layers, 4 heads
- **Context**: 128 tokens max
- **Tokenizer**: 44 character-level tokens (letters, digits, symbols)

### Training Setup
- **Batch size**: 64 (critical!)
- **Learning rate**: 5e-4 with linear warmup
- **Epochs**: Varied by experiment (20-100)
- **Loss masking**: Answer-only (train on outputs, not prompts)

### Data Preparation
- **Cities**: 5,075 with population ≥100k
- **Train/Val split**: 80/20 for generalization tests
- **Coordinate encoding**: Integers (0-6283 for longitude, 0-3141 for latitude)
- **Distance unit**: Kilometers (rounded)

## What's Next: The Perfect Testbed for Representation Research

With training finally working, we discovered this setup is ideal for our broader research program:

### For Representation Formation Studies:
1. **Modularity Analysis**: Do geographic representations form as discrete modules (Europe, Asia) or entangled patterns?
2. **Scaling Dynamics**: How quickly do clean representations emerge as we increase data/model size?
3. **Task Transfer**: Do distance and location tasks create compatible or competing representations?
4. **Hierarchical Discovery**: Can we detect when the model discovers countries/continents without supervision?

### For Controlled Experiments:
1. **Data Ablations**: Remove specific continents and study representation reorganization
2. **Architecture Sweeps**: Test how model width/depth affects geographic representation quality
3. **Training Dynamics**: Track representation formation epoch-by-epoch during training
4. **Intervention Studies**: What happens when we artificially modify representations mid-training?

This city coordinate task gives us everything we need: controllability, visualization, scaling potential, and real structure. The answers are already looking fascinating—but that's a story for the next posts.

## Key Takeaways for Representation Research

This work established several crucial principles for studying representation formation:

1. **Scale Matters Differently**: For small datasets, gradient update frequency matters more than data diversity
2. **Synthetic Tasks Can Be Rich**: Geographic coordinates provide controllable complexity without sacrificing meaningful structure  
3. **Visualization Enables Science**: Being able to plot representations on maps makes causal analysis tractable
4. **Controls Are Essential**: Random mapping datasets isolate training dynamics from task structure
5. **Simple Setups Reveal Fundamentals**: Sometimes the most "trivial" tasks expose the deepest principles

## Why This Approach Could Transform Representation Research

Most interpretability research studies representations after models are fully trained. But we've built a system for studying representation formation in real-time:

- **Causal Interventions**: We can ablate data, modify architectures, and track effects
- **Ground Truth Verification**: Unlike abstract synthetic data, we can verify representations make geographic sense
- **Scalable Complexity**: From 100 cities (simple) to 10,000+ cities (complex) with the same framework
- **Multiple Analysis Levels**: Individual neurons, layer-wise evolution, cross-task comparison

## A Research Philosophy: Start Simple, Scale Systematically

This project embodies our lab's approach to representation research:
- **Fail fast**: Debug thoroughly on simple cases before scaling
- **Visualize everything**: If you can't plot it, you don't understand it  
- **Control for everything**: Every hypothesis needs a matched control condition
- **Scale systematically**: Understand dynamics at small scales before going large

The journey from "why won't it memorize?" to "how do world models form?" taught us that sometimes the most fundamental insights come from the simplest tasks.

---

*Next: How a 6-layer transformer trained only on city distances develops an internal map of the world, complete with continental boundaries it was never taught—and what this reveals about representation modularity.*