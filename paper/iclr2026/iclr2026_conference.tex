
\documentclass{article} % For LaTeX2e
\usepackage{iclr2026_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


\title{On the Role of World Representations for Generalization}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Core Francisco Park \\
Harvard University$^1$ \\
CBS-NTT Program in Physics of Intelligence$^2$ \\
Cambridge, MA 02138, USA \\
\texttt{corefranciscopark@g.harvard.edu}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Recent work has shown that pretrained neural networks develop representational structures mirroring those of the real world. However, most studies analyze only pretrained models, leaving open the question of how such structures emerge during training and how they support generalization. We address this gap using a controllable synthetic setup in which the underlying world is fixed but the data generation process can be systematically varied. With this setup, first, we show that Transformers learn the world structure in later layers and then propagate it backward. Second, we identify conditions under which the same world yields---or fails to yield---clear linear representations depending on the data generation process. Third, we probe the role of these internal representations for generalization by inducing consistent changes in the world and fine-tuning on the resulting data. Our controllable framework paves a path toward understanding the origins and functions of world representations, linking observational findings to practical insights.
\end{abstract}

\section{Introduction}

% Introduction content will go here

\section{Related Works}

\textbf{Internal Representations.} Recent work has shown that neural networks develop structured internal representations that mirror the organization of the external world \citep{Bengio+chapter2007}. Studies have demonstrated that these representations emerge naturally during training, with networks learning to encode semantic relationships, hierarchical structures, and compositional patterns. Various probing techniques have been developed to analyze these representations, revealing that deeper layers tend to capture more abstract and invariant features. However, the precise mechanisms by which these representations form and how they relate to the training dynamics remain open questions. [PLACEHOLDER: Add more specific citations about representation learning, geometric structures in neural networks, and emergent properties]

\textbf{Interpretability.} The field of neural network interpretability has developed numerous methods to understand what models learn and how they make decisions \citep{Hinton06}. Mechanistic interpretability approaches aim to reverse-engineer the computational structures within networks, identifying circuits and modules responsible for specific behaviors. Linear probing and other diagnostic techniques have revealed that networks often learn human-interpretable concepts in their intermediate layers. Recent advances in visualization techniques and attribution methods have provided insights into the feature detectors learned by different architectures. [PLACEHOLDER: Expand with specific interpretability methods, circuit discovery techniques, and relevant benchmarks]

\textbf{Pretraining \& Fine-Tuning.} The pretraining-finetuning paradigm has become central to modern deep learning, particularly in natural language processing and computer vision \citep{goodfellow2016deep}. Large-scale pretraining on diverse datasets enables models to learn general-purpose representations that transfer effectively to downstream tasks. Studies have shown that fine-tuning can rapidly adapt these representations while preserving much of the knowledge acquired during pretraining. The relationship between pretraining objectives, data distribution, and downstream performance remains an active area of research. Understanding how representations shift during fine-tuning and which features are preserved versus modified is crucial for improving transfer learning. [PLACEHOLDER: Add details about specific pretraining methods, transfer learning theory, and empirical findings about representation dynamics]

\section{Setup: A Model System of Worlds}

% Setup content will go here

\section{Results: Formation of World Representations During Pretraining}

% Results section 1 content will go here

\section{Results: Fine-tuning's Representational Shift Predicts Downstream Generalization}


% Results section 2 content will go here

\section{Discussion}

% Discussion content will go here


% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.


\bibliography{iclr2026_conference}
\bibliographystyle{iclr2026_conference}

% \appendix
% \section{Appendix}
% You may include other additional sections here.


\end{document}
