\begin{thebibliography}{97}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achille et~al.(2019)Achille, Rovere, and
  Soatto]{achille2019criticallearningperiodsdeep}
Alessandro Achille, Matteo Rovere, and Stefano Soatto.
\newblock Critical learning periods in deep neural networks, 2019.
\newblock URL \url{https://arxiv.org/abs/1711.08856}.

\bibitem[Aghajanyan et~al.(2021)Aghajanyan, Gupta, Shrivastava, Chen,
  Zettlemoyer, and Gupta]{aghajanyan2021muppetmassivemultitaskrepresentations}
Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke
  Zettlemoyer, and Sonal Gupta.
\newblock Muppet: Massive multi-task representations with pre-finetuning, 2021.
\newblock URL \url{https://arxiv.org/abs/2101.11038}.

\bibitem[Allen-Zhu \& Li(2023{\natexlab{a}})Allen-Zhu and Li]{allen2023physics}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Physics of language models: Part 3.1, knowledge storage and
  extraction.
\newblock \emph{arXiv preprint arXiv:2309.14316}, 2023{\natexlab{a}}.

\bibitem[Allen-Zhu \& Li(2023{\natexlab{b}})Allen-Zhu and
  Li]{allen2023physics1}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Physics of language models: Part 1, learning hierarchical language
  structures.
\newblock \emph{ArXiv e-prints, abs/2305.13673, May}, 2023{\natexlab{b}}.

\bibitem[{Anthropic AI}(2023)]{towardsmonosemanticity}
{Anthropic AI}.
\newblock \emph{Towards Monosemanticity: Decomposing Language Models With
  Dictionary Learning}, 2023.
\newblock \url{https://transformer-circuits.pub/2023/monosemantic-features}.

\bibitem[Arditi et~al.(2024)Arditi, Obeso, Syed, Paleka, Panickssery, Gurnee,
  and Nanda]{arditi2024refusal}
Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Panickssery, Wes
  Gurnee, and Neel Nanda.
\newblock Refusal in language models is mediated by a single direction.
\newblock \emph{arXiv preprint arXiv:2406.11717}, 2024.

\bibitem[Bachmann \& Nagarajan(2025)Bachmann and
  Nagarajan]{bachmann2025pitfallsnexttokenprediction}
Gregor Bachmann and Vaishnavh Nagarajan.
\newblock The pitfalls of next-token prediction, 2025.
\newblock URL \url{https://arxiv.org/abs/2403.06963}.

\bibitem[Behrouz et~al.(2024)Behrouz, Zhong, and
  Mirrokni]{behrouz2024titanslearningmemorizetest}
Ali Behrouz, Peilin Zhong, and Vahab Mirrokni.
\newblock Titans: Learning to memorize at test time, 2024.
\newblock URL \url{https://arxiv.org/abs/2501.00663}.

\bibitem[Bengio et~al.(2014)Bengio, Courville, and
  Vincent]{bengio2014representationlearningreviewnew}
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
\newblock Representation learning: A review and new perspectives, 2014.
\newblock URL \url{https://arxiv.org/abs/1206.5538}.

\bibitem[Berglund et~al.(2024)Berglund, Tong, Kaufmann, Balesni, Stickland,
  Korbak, and Evans]{berglund2024reversalcursellmstrained}
Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa~Cooper Stickland,
  Tomasz Korbak, and Owain Evans.
\newblock The reversal curse: Llms trained on "a is b" fail to learn "b is a",
  2024.
\newblock URL \url{https://arxiv.org/abs/2309.12288}.

\bibitem[Betley et~al.(2025)Betley, Tan, Warncke, Sztyber-Betley, Bao, Soto,
  Labenz, and Evans]{betley2025emergentmisalignmentnarrowfinetuning}
Jan Betley, Daniel Tan, Niels Warncke, Anna Sztyber-Betley, Xuchan Bao, Martín
  Soto, Nathan Labenz, and Owain Evans.
\newblock Emergent misalignment: Narrow finetuning can produce broadly
  misaligned llms, 2025.
\newblock URL \url{https://arxiv.org/abs/2502.17424}.

\bibitem[Bigelow et~al.(2025)Bigelow, Wurgaft, Wang, Goodman, Ullman, Tanaka,
  and Lubana]{bigelow2025beliefdynamicsrevealdual}
Eric Bigelow, Daniel Wurgaft, YingQiao Wang, Noah Goodman, Tomer Ullman,
  Hidenori Tanaka, and Ekdeep~Singh Lubana.
\newblock Belief dynamics reveal the dual nature of in-context learning and
  activation steering, 2025.
\newblock URL \url{https://arxiv.org/abs/2511.00617}.

\bibitem[Bronstein et~al.(2021)Bronstein, Bruna, Cohen, and
  Veličković]{bronstein2021geometricdeeplearninggrids}
Michael~M. Bronstein, Joan Bruna, Taco Cohen, and Petar Veličković.
\newblock Geometric deep learning: Grids, groups, graphs, geodesics, and
  gauges, 2021.
\newblock URL \url{https://arxiv.org/abs/2104.13478}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Caruana(1997)]{caruana1997multitask}
Rich Caruana.
\newblock Multitask learning.
\newblock \emph{Machine learning}, 28\penalty0 (1):\penalty0 41--75, 1997.

\bibitem[Casademunt et~al.(2025)Casademunt, Juang, Karvonen, Marks,
  Rajamanoharan, and
  Nanda]{casademunt2025steeringoutofdistributiongeneralizationconcept}
Helena Casademunt, Caden Juang, Adam Karvonen, Samuel Marks, Senthooran
  Rajamanoharan, and Neel Nanda.
\newblock Steering out-of-distribution generalization with concept ablation
  fine-tuning, 2025.
\newblock URL \url{https://arxiv.org/abs/2507.16795}.

\bibitem[Chan et~al.(2022)Chan, Santoro, Lampinen, Wang, Singh, Richemond,
  McClelland, and Hill]{chan2022datadistributionalpropertiesdrive}
Stephanie C.~Y. Chan, Adam Santoro, Andrew~K. Lampinen, Jane~X. Wang, Aaditya
  Singh, Pierre~H. Richemond, Jay McClelland, and Felix Hill.
\newblock Data distributional properties drive emergent in-context learning in
  transformers, 2022.
\newblock URL \url{https://arxiv.org/abs/2205.05055}.

\bibitem[Charakorn et~al.(2025)Charakorn, Cetin, Tang, and
  Lange]{charakorn2025texttolorainstanttransformeradaption}
Rujikorn Charakorn, Edoardo Cetin, Yujin Tang, and Robert~Tjarko Lange.
\newblock Text-to-lora: Instant transformer adaption, 2025.
\newblock URL \url{https://arxiv.org/abs/2506.06105}.

\bibitem[Chen et~al.(2024)Chen, Fang, Xia, Liu, Durme, Zettlemoyer, Gao, and
  Cheng]{chen2024generativeadaptercontextualizinglanguage}
Tong Chen, Hao Fang, Patrick Xia, Xiaodong Liu, Benjamin~Van Durme, Luke
  Zettlemoyer, Jianfeng Gao, and Hao Cheng.
\newblock Generative adapter: Contextualizing language models in parameters
  with a single forward pass, 2024.
\newblock URL \url{https://arxiv.org/abs/2411.05877}.

\bibitem[Cohen \& Welling(2016)Cohen and
  Welling]{cohen2016groupequivariantconvolutionalnetworks}
Taco~S. Cohen and Max Welling.
\newblock Group equivariant convolutional networks, 2016.
\newblock URL \url{https://arxiv.org/abs/1602.07576}.

\bibitem[Csord{\'a}s et~al.(2024)Csord{\'a}s, Potts, Manning, and
  Geiger]{csordas2024recurrent}
R{\'o}bert Csord{\'a}s, Christopher Potts, Christopher~D Manning, and Atticus
  Geiger.
\newblock Recurrent neural networks learn to store and generate sequences using
  non-linear representations.
\newblock \emph{arXiv preprint arXiv:2408.10920}, 2024.

\bibitem[Demircan et~al.(2024)Demircan, Saanum, Jagadish, Binz, and
  Schulz]{demircan2024sparseautoencodersrevealtemporal}
Can Demircan, Tankred Saanum, Akshay~K. Jagadish, Marcel Binz, and Eric Schulz.
\newblock Sparse autoencoders reveal temporal difference learning in large
  language models, 2024.
\newblock URL \url{https://arxiv.org/abs/2410.01280}.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dohare et~al.(2024)Dohare, Hernandez-Garcia, Rahman, Mahmood, and
  Sutton]{dohare2024maintainingplasticitydeepcontinual}
Shibhansh Dohare, J.~Fernando Hernandez-Garcia, Parash Rahman, A.~Rupam
  Mahmood, and Richard~S. Sutton.
\newblock Maintaining plasticity in deep continual learning, 2024.
\newblock URL \url{https://arxiv.org/abs/2306.13812}.

\bibitem[Engels et~al.(2024)Engels, Liao, Michaud, Gurnee, and
  Tegmark]{engels2024languagemodelfeatureslinear}
Joshua Engels, Isaac Liao, Eric~J. Michaud, Wes Gurnee, and Max Tegmark.
\newblock Not all language model features are linear, 2024.
\newblock URL \url{https://arxiv.org/abs/2405.14860}.

\bibitem[Fu et~al.(2025)Fu, Bonnen, Guillory, and
  Darrell]{fu2025hiddenplainsightvlms}
Stephanie Fu, Tyler Bonnen, Devin Guillory, and Trevor Darrell.
\newblock Hidden in plain sight: Vlms overlook their visual representations,
  2025.
\newblock URL \url{https://arxiv.org/abs/2506.08008}.

\bibitem[Fukushima(1980)]{fukushima1980neocognitron}
Kunihiko Fukushima.
\newblock Neocognitron: A self-organizing neural network model for a mechanism
  of pattern recognition unaffected by shift in position.
\newblock \emph{Biological cybernetics}, 36\penalty0 (4):\penalty0 193--202,
  1980.

\bibitem[Ge et~al.(2025)Ge, Shu, Wu, Zhou, He, and
  Qiu]{ge2025evolutionconceptslanguagemodel}
Xuyang Ge, Wentao Shu, Jiaxing Wu, Yunhua Zhou, Zhengfu He, and Xipeng Qiu.
\newblock Evolution of concepts in language model pre-training, 2025.
\newblock URL \url{https://arxiv.org/abs/2509.17196}.

\bibitem[Gopalani \& Hu(2025)Gopalani and
  Hu]{gopalani2025happenslossplateauunderstanding}
Pulkit Gopalani and Wei Hu.
\newblock What happens during the loss plateau? understanding abrupt learning
  in transformers, 2025.
\newblock URL \url{https://arxiv.org/abs/2506.13688}.

\bibitem[Gurnee \& Tegmark(2023)Gurnee and Tegmark]{gurnee2023language}
Wes Gurnee and Max Tegmark.
\newblock Language models represent space and time.
\newblock \emph{arXiv preprint arXiv:2310.02207}, 2023.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition, 2015.

\bibitem[Higgins et~al.(2017)Higgins, Matthey, Pal, Burgess, Glorot, Botvinick,
  Mohamed, and Lerchner]{higgins2016beta}
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot,
  Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner.
\newblock {beta-vae: Learning basic visual concepts with a constrained
  variational framework}.
\newblock \emph{In Proc.\ Int.\ Conf.\ on Learning Representations (ICLR)},
  2017.

\bibitem[Hindupur et~al.(2025)Hindupur, Lubana, Fel, and
  Ba]{hindupur2025projectingassumptionsdualitysparse}
Sai Sumedh~R. Hindupur, Ekdeep~Singh Lubana, Thomas Fel, and Demba Ba.
\newblock Projecting assumptions: The duality between sparse autoencoders and
  concept geometry, 2025.
\newblock URL \url{https://arxiv.org/abs/2503.01822}.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Hoffmann et~al.(2024)Hoffmann, Schrodi, Bratulić, Behrmann, Fischer,
  and Brox]{hoffmann2024eurekamomentstransformersmultisteptasks}
David~T. Hoffmann, Simon Schrodi, Jelena Bratulić, Nadine Behrmann, Volker
  Fischer, and Thomas Brox.
\newblock Eureka-moments in transformers: Multi-step tasks reveal softmax
  induced optimization problems, 2024.
\newblock URL \url{https://arxiv.org/abs/2310.12956}.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and
  Chen]{hu2021loralowrankadaptationlarge}
Edward~J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
  Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models, 2021.
\newblock URL \url{https://arxiv.org/abs/2106.09685}.

\bibitem[Hubel \& Wiesel(1962)Hubel and Wiesel]{hubel1962receptive}
David~H Hubel and Torsten~N Wiesel.
\newblock Receptive fields, binocular interaction and functional architecture
  in the cat's visual cortex.
\newblock \emph{The Journal of physiology}, 160\penalty0 (1):\penalty0 106,
  1962.

\bibitem[Huh et~al.(2024)Huh, Cheung, Wang, and
  Isola]{huh2024platonicrepresentationhypothesis}
Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola.
\newblock The platonic representation hypothesis, 2024.
\newblock URL \url{https://arxiv.org/abs/2405.07987}.

\bibitem[Ilharco et~al.(2023)Ilharco, Ribeiro, Wortsman, Gururangan, Schmidt,
  Hajishirzi, and Farhadi]{ilharco2023editingmodelstaskarithmetic}
Gabriel Ilharco, Marco~Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan,
  Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi.
\newblock Editing models with task arithmetic, 2023.
\newblock URL \url{https://arxiv.org/abs/2212.04089}.

\bibitem[Jain et~al.(2023)Jain, Kirk, Lubana, Dick, Tanaka, Grefenstette,
  Rockt{\"a}schel, and Krueger]{jain2023mechanistically}
Samyak Jain, Robert Kirk, Ekdeep~Singh Lubana, Robert~P Dick, Hidenori Tanaka,
  Edward Grefenstette, Tim Rockt{\"a}schel, and David~Scott Krueger.
\newblock Mechanistically analyzing the effects of fine-tuning on procedurally
  defined tasks.
\newblock \emph{arXiv preprint arXiv:2311.12786}, 2023.

\bibitem[Kim et~al.(2025)Kim, Kwon, Choi, Park, Cho, Lee, and
  Ryu]{kim2025taskdiversityshortensicl}
Jaeyeon Kim, Sehyun Kwon, Joo~Young Choi, Jongho Park, Jaewoong Cho, Jason~D.
  Lee, and Ernest~K. Ryu.
\newblock Task diversity shortens the icl plateau, 2025.
\newblock URL \url{https://arxiv.org/abs/2410.05448}.

\bibitem[Kornblith et~al.(2019)Kornblith, Norouzi, Lee, and
  Hinton]{kornblithcka}
Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton.
\newblock {Similarity of Neural Network Representations Revisited}.
\newblock In \emph{Proc.\ of the 36th Proc.\ Int.\ Conf.\ on Machine Learning
  (ICML)}, Proc.\ of Machine Learning Research. PMLR, 09--15 Jun 2019.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock \emph{Advances in neural information processing systems}, 25, 2012.

\bibitem[Kumar et~al.(2025)Kumar, Clune, Lehman, and
  Stanley]{kumar2025questioningrepresentationaloptimismdeep}
Akarsh Kumar, Jeff Clune, Joel Lehman, and Kenneth~O. Stanley.
\newblock Questioning representational optimism in deep learning: The fractured
  entangled representation hypothesis, 2025.
\newblock URL \url{https://arxiv.org/abs/2505.11581}.

\bibitem[Kumar et~al.(2022)Kumar, Raghunathan, Jones, Ma, and
  Liang]{kumar2022finetuningdistortpretrainedfeatures}
Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang.
\newblock Fine-tuning can distort pretrained features and underperform
  out-of-distribution, 2022.
\newblock URL \url{https://arxiv.org/abs/2202.10054}.

\bibitem[Lampinen et~al.(2025)Lampinen, Chaudhry, Chan, Wild, Wan, Ku,
  Bornschein, Pascanu, Shanahan, and
  McClelland]{lampinen2025generalizationlanguagemodelsincontext}
Andrew~K. Lampinen, Arslan Chaudhry, Stephanie C.~Y. Chan, Cody Wild, Diane
  Wan, Alex Ku, Jörg Bornschein, Razvan Pascanu, Murray Shanahan, and James~L.
  McClelland.
\newblock On the generalization of language models from in-context learning and
  finetuning: a controlled study, 2025.
\newblock URL \url{https://arxiv.org/abs/2505.00661}.

\bibitem[Lee et~al.(2024)Lee, Bai, Pres, Wattenberg, Kummerfeld, and
  Mihalcea]{lee2024mechanistic}
Andrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wattenberg, Jonathan~K Kummerfeld,
  and Rada Mihalcea.
\newblock A mechanistic understanding of alignment algorithms: A case study on
  dpo and toxicity.
\newblock In \emph{Forty-first International Conference on Machine Learning},
  2024.
\newblock URL \url{https://arxiv.org/abs/2401.01967}.

\bibitem[Lee et~al.(2025)Lee, Sun, Wendler, Viégas, and
  Wattenberg]{lee2025geometryselfverificationtaskspecificreasoning}
Andrew Lee, Lihao Sun, Chris Wendler, Fernanda Viégas, and Martin Wattenberg.
\newblock The geometry of self-verification in a task-specific reasoning model,
  2025.
\newblock URL \url{https://arxiv.org/abs/2504.14379}.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and
  Constant]{lester2021powerscaleparameterefficientprompt}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning, 2021.
\newblock URL \url{https://arxiv.org/abs/2104.08691}.

\bibitem[Li et~al.(2022)Li, Hopkins, Bau, Vi{\'e}gas, Pfister, and
  Wattenberg]{li2022emergent}
Kenneth Li, Aspen~K Hopkins, David Bau, Fernanda Vi{\'e}gas, Hanspeter Pfister,
  and Martin Wattenberg.
\newblock Emergent world representations: Exploring a sequence model trained on
  a synthetic task.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2022.

\bibitem[Li et~al.(2025)Li, Agrawal, Ghosh, Teru, Lajoie, and
  Richards]{li2025tracing}
Melody~Zixuan Li, Kumar~Krishna Agrawal, Arna Ghosh, Komal~Kumar Teru,
  Guillaume Lajoie, and Blake~Aaron Richards.
\newblock Tracing the representation geometry of language models from
  pretraining to post-training.
\newblock In \emph{High-dimensional Learning Dynamics 2025}, 2025.
\newblock URL \url{https://openreview.net/forum?id=9nKmDLXg9v}.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and
  Hutter]{loshchilov2019decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization, 2019.

\bibitem[Lubana et~al.(2025)Lubana, Rager, Hindupur, Costa, Tuckute, Patel,
  Murthy, Fel, Wurgaft, Bigelow, Lin, Ba, Wattenberg, Viegas, Weber, and
  Mueller]{lubana2025priorstimemissinginductive}
Ekdeep~Singh Lubana, Can Rager, Sai Sumedh~R. Hindupur, Valerie Costa, Greta
  Tuckute, Oam Patel, Sonia~Krishna Murthy, Thomas Fel, Daniel Wurgaft, Eric~J.
  Bigelow, Johnny Lin, Demba Ba, Martin Wattenberg, Fernanda Viegas, Melanie
  Weber, and Aaron Mueller.
\newblock Priors in time: Missing inductive biases for language model
  interpretability, 2025.
\newblock URL \url{https://arxiv.org/abs/2511.01836}.

\bibitem[Malladi et~al.(2024)Malladi, Gao, Nichani, Damian, Lee, Chen, and
  Arora]{malladi2024finetuninglanguagemodelsjust}
Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason~D. Lee, Danqi
  Chen, and Sanjeev Arora.
\newblock Fine-tuning language models with just forward passes, 2024.
\newblock URL \url{https://arxiv.org/abs/2305.17333}.

\bibitem[Marks \& Tegmark(2024)Marks and
  Tegmark]{marks2024geometrytruthemergentlinear}
Samuel Marks and Max Tegmark.
\newblock The geometry of truth: Emergent linear structure in large language
  model representations of true/false datasets, 2024.
\newblock URL \url{https://arxiv.org/abs/2310.06824}.

\bibitem[McCloskey \& Cohen(1989)McCloskey and
  Cohen]{mccloskey1989catastrophic}
Michael McCloskey and Neal~J Cohen.
\newblock Catastrophic interference in connectionist networks: The sequential
  learning problem.
\newblock In \emph{Psychology of learning and motivation}, volume~24, pp.\
  109--165. Elsevier, 1989.

\bibitem[Menon et~al.(2025)Menon, Shrivastava, Krueger, and
  Lubana]{menon2025analyzinginabilitiessaesformal}
Abhinav Menon, Manish Shrivastava, David Krueger, and Ekdeep~Singh Lubana.
\newblock Analyzing (in)abilities of saes via formal languages, 2025.
\newblock URL \url{https://arxiv.org/abs/2410.11767}.

\bibitem[Michaud et~al.(2023)Michaud, Liu, Girit, and
  Tegmark]{michaud2023quantization}
Eric~J Michaud, Ziming Liu, Uzay Girit, and Max Tegmark.
\newblock The quantization model of neural scaling.
\newblock \emph{arXiv preprint arXiv:2303.13506}, 2023.

\bibitem[Minder et~al.(2025)Minder, Dumas, Juang, Chugtai, and
  Nanda]{minder2025overcomingsparsityartifactscrosscoders}
Julian Minder, Clément Dumas, Caden Juang, Bilal Chugtai, and Neel Nanda.
\newblock Overcoming sparsity artifacts in crosscoders to interpret
  chat-tuning, 2025.
\newblock URL \url{https://arxiv.org/abs/2504.02922}.

\bibitem[Mircea et~al.(2025)Mircea, Chakraborty, Chitsazan, Naphade, Sahu,
  Rish, and Lobacheva]{mircea2025trainingdynamicsunderlyinglanguage}
Andrei Mircea, Supriyo Chakraborty, Nima Chitsazan, Milind Naphade, Sambit
  Sahu, Irina Rish, and Ekaterina Lobacheva.
\newblock Training dynamics underlying language model scaling laws: Loss
  deceleration and zero-sum learning, 2025.
\newblock URL \url{https://arxiv.org/abs/2506.05447}.

\bibitem[Nanda et~al.(2023{\natexlab{a}})Nanda, Chan, Lieberum, Smith, and
  Steinhardt]{nanda2023progressmeasuresgrokkingmechanistic}
Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt.
\newblock Progress measures for grokking via mechanistic interpretability,
  2023{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2301.05217}.

\bibitem[Nanda et~al.(2023{\natexlab{b}})Nanda, Lee, and
  Wattenberg]{nanda2023emergent}
Neel Nanda, Andrew Lee, and Martin Wattenberg.
\newblock Emergent linear representations in world models of self-supervised
  sequence models.
\newblock In \emph{Proceedings of the 6th BlackboxNLP Workshop: Analyzing and
  Interpreting Neural Networks for NLP}, pp.\  16--30, 2023{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2309.00941}.

\bibitem[Nishi et~al.(2024)Nishi, Okawa, Ramesh, Khona, Lubana, and
  Tanaka]{nishi2024representation}
Kento Nishi, Maya Okawa, Rahul Ramesh, Mikail Khona, Ekdeep~Singh Lubana, and
  Hidenori Tanaka.
\newblock Representation shattering in transformers: A synthetic study with
  knowledge editing.
\newblock \emph{arXiv preprint arXiv:2410.17194}, 2024.

\bibitem[Okawa et~al.(2024)Okawa, Lubana, Dick, and
  Tanaka]{okawa2024compositional}
Maya Okawa, Ekdeep~Singh Lubana, Robert~P. Dick, and Hidenori Tanaka.
\newblock Compositional abilities emerge multiplicatively: Exploring diffusion
  models on a synthetic task, 2024.

\bibitem[Olah et~al.(2017)Olah, Mordvintsev, and Schubert]{olah2017feature}
Chris Olah, Alexander Mordvintsev, and Ludwig Schubert.
\newblock Feature visualization.
\newblock \emph{Distill}, 2017.
\newblock \doi{10.23915/distill.00007}.
\newblock https://distill.pub/2017/feature-visualization.

\bibitem[{OpenDataSoft / GeoNames}(2025)]{geonames_all_cities_1000}
{OpenDataSoft / GeoNames}.
\newblock Geonames – all cities with a population > 1000.
\newblock
  \url{https://public.opendatasoft.com/explore/dataset/geonames-all-cities-with-a-population-1000},
  2025.
\newblock Accessed: 2025.

\bibitem[Park et~al.(2024{\natexlab{a}})Park, Lee, Lubana, Yang, Okawa, Nishi,
  Wattenberg, and Tanaka]{park2024iclrincontextlearningrepresentations}
Core~Francisco Park, Andrew Lee, Ekdeep~Singh Lubana, Yongyi Yang, Maya Okawa,
  Kento Nishi, Martin Wattenberg, and Hidenori Tanaka.
\newblock Iclr: In-context learning of representations, 2024{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2501.00070}.

\bibitem[Park et~al.(2024{\natexlab{b}})Park, Lubana, Pres, and
  Tanaka]{park2024competition}
Core~Francisco Park, Ekdeep~Singh Lubana, Itamar Pres, and Hidenori Tanaka.
\newblock Competition dynamics shape algorithmic phases of in-context learning.
\newblock \emph{arXiv preprint arXiv:2412.01003}, 2024{\natexlab{b}}.

\bibitem[Park et~al.(2024{\natexlab{c}})Park, Okawa, Lee, Lubana, and
  Tanaka]{park2024emergencehiddencapabilitiesexploring}
Core~Francisco Park, Maya Okawa, Andrew Lee, Ekdeep~Singh Lubana, and Hidenori
  Tanaka.
\newblock Emergence of hidden capabilities: Exploring learning dynamics in
  concept space, 2024{\natexlab{c}}.
\newblock URL \url{https://arxiv.org/abs/2406.19370}.

\bibitem[Park et~al.(2025)Park, Zhang, and
  Tanaka]{park2025textitnewnewssystem2finetuning}
Core~Francisco Park, Zechen Zhang, and Hidenori Tanaka.
\newblock $\textit{New News}$: System-2 fine-tuning for robust integration of
  new knowledge, 2025.
\newblock URL \url{https://arxiv.org/abs/2505.01812}.

\bibitem[Pearce et~al.(2025)Pearce, Simon, Byun, and Balsam]{pearce2025tree}
Michael Pearce, Elana Simon, Michael Byun, and Daniel Balsam.
\newblock Finding the tree of life in evo 2.
\newblock \emph{Goodfire Research}, August 2025.
\newblock Correspondence to michael@goodfire.ai.

\bibitem[Pezeshki et~al.(2021)Pezeshki, Kaba, Bengio, Courville, Precup, and
  Lajoie]{pezeshki2021gradient}
Mohammad Pezeshki, Oumar Kaba, Yoshua Bengio, Aaron~C Courville, Doina Precup,
  and Guillaume Lajoie.
\newblock {Gradient starvation: A learning proclivity in neural networks}.
\newblock \emph{Adv.\ in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem[Qin et~al.(2025)Qin, Park, Kwun, Walsman, Malach, Anand, Tanaka, and
  Alvarez-Melis]{qin2025decomposingelementsproblemsolving}
Tian Qin, Core~Francisco Park, Mujin Kwun, Aaron Walsman, Eran Malach, Nikhil
  Anand, Hidenori Tanaka, and David Alvarez-Melis.
\newblock Decomposing elements of problem solving: What "math" does rl teach?,
  2025.
\newblock URL \url{https://arxiv.org/abs/2505.22756}.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, Sutskever,
  et~al.]{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et~al.
\newblock Improving language understanding by generative pre-training, 2018.

\bibitem[Raventós et~al.(2023)Raventós, Paul, Chen, and
  Ganguli]{raventós2023pretrainingtaskdiversityemergence}
Allan Raventós, Mansheej Paul, Feng Chen, and Surya Ganguli.
\newblock Pretraining task diversity and the emergence of non-bayesian
  in-context learning for regression, 2023.
\newblock URL \url{https://arxiv.org/abs/2306.15063}.

\bibitem[Reddy(2023)]{reddy2023mechanisticbasisdatadependence}
Gautam Reddy.
\newblock The mechanistic basis of data dependence and abrupt learning in an
  in-context classification task, 2023.
\newblock URL \url{https://arxiv.org/abs/2312.03002}.

\bibitem[Rosenblatt(1958)]{rosenblatt1958perceptron}
Frank Rosenblatt.
\newblock The perceptron: a probabilistic model for information storage and
  organization in the brain.
\newblock \emph{Psychological review}, 65\penalty0 (6):\penalty0 386, 1958.

\bibitem[Rumelhart et~al.(1986)Rumelhart, Hinton, and
  Williams]{rumelhart1986learning}
David~E Rumelhart, Geoffrey~E Hinton, and Ronald~J Williams.
\newblock Learning representations by back-propagating errors.
\newblock \emph{nature}, 323\penalty0 (6088):\penalty0 533--536, 1986.

\bibitem[Schlag et~al.(2021)Schlag, Irie, and
  Schmidhuber]{schlag2021lineartransformerssecretlyfast}
Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber.
\newblock Linear transformers are secretly fast weight programmers, 2021.
\newblock URL \url{https://arxiv.org/abs/2102.11174}.

\bibitem[Shah et~al.(2020)Shah, Tamuly, Raghunathan, Jain, and
  Netrapalli]{shah2020pitfallssimplicitybiasneural}
Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth
  Netrapalli.
\newblock The pitfalls of simplicity bias in neural networks, 2020.
\newblock URL \url{https://arxiv.org/abs/2006.07710}.

\bibitem[Shai et~al.(2025)Shai, Marzen, Teixeira, Oldenziel, and
  Riechers]{shai2025transformersrepresentbeliefstate}
Adam~S. Shai, Sarah~E. Marzen, Lucas Teixeira, Alexander~Gietelink Oldenziel,
  and Paul~M. Riechers.
\newblock Transformers represent belief state geometry in their residual
  stream, 2025.
\newblock URL \url{https://arxiv.org/abs/2405.15943}.

\bibitem[Singh et~al.(2024)Singh, Moskovitz, Hill, Chan, and
  Saxe]{singh2024needsrightinductionhead}
Aaditya~K. Singh, Ted Moskovitz, Felix Hill, Stephanie C.~Y. Chan, and
  Andrew~M. Saxe.
\newblock What needs to go right for an induction head? a mechanistic study of
  in-context learning circuits and their formation, 2024.
\newblock URL \url{https://arxiv.org/abs/2404.07129}.

\bibitem[Templeton et~al.(2024)Templeton, Conerly, Marcus, Lindsey, Bricken,
  Chen, Pearce, Citro, Ameisen, Jones, Cunningham, Turner, McDougall,
  MacDiarmid, Freeman, Sumers, Rees, Batson, Jermyn, Carter, Olah, and
  Henighan]{templeton2024scaling}
Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken,
  Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy
  Cunningham, Nicholas~L Turner, Callum McDougall, Monte MacDiarmid, C.~Daniel
  Freeman, Theodore~R. Sumers, Edward Rees, Joshua Batson, Adam Jermyn, Shan
  Carter, Chris Olah, and Tom Henighan.
\newblock Scaling monosemanticity: Extracting interpretable features from
  claude 3 sonnet.
\newblock \emph{Transformer Circuits Thread}, 2024.
\newblock URL
  \url{https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}.

\bibitem[Treutlein et~al.(2024)Treutlein, Choi, Betley, Marks, Anil, Grosse,
  and Evans]{treutlein2024connectingdotsllmsinfer}
Johannes Treutlein, Dami Choi, Jan Betley, Samuel Marks, Cem Anil, Roger
  Grosse, and Owain Evans.
\newblock Connecting the dots: Llms can infer and verbalize latent structure
  from disparate training data, 2024.
\newblock URL \url{https://arxiv.org/abs/2406.14546}.

\bibitem[Vafa et~al.(2025)Vafa, Chang, Rambachan, and
  Mullainathan]{vafa2025foundationmodelfoundusing}
Keyon Vafa, Peter~G. Chang, Ashesh Rambachan, and Sendhil Mullainathan.
\newblock What has a foundation model found? using inductive bias to probe for
  world models, 2025.
\newblock URL \url{https://arxiv.org/abs/2507.06952}.

\bibitem[Wang et~al.(2025)Wang, Engels, Clive-Griffin, Rajamanoharan, and
  Nanda]{wang2025simplemechanisticexplanationsoutofcontext}
Atticus Wang, Joshua Engels, Oliver Clive-Griffin, Senthooran Rajamanoharan,
  and Neel Nanda.
\newblock Simple mechanistic explanations for out-of-context reasoning, 2025.
\newblock URL \url{https://arxiv.org/abs/2507.08218}.

\bibitem[Ward et~al.(2025)Ward, Lin, Venhoff, and
  Nanda]{ward2025reasoningfinetuningrepurposeslatentrepresentations}
Jake Ward, Chuqiao Lin, Constantin Venhoff, and Neel Nanda.
\newblock Reasoning-finetuning repurposes latent representations in base
  models, 2025.
\newblock URL \url{https://arxiv.org/abs/2507.12638}.

\bibitem[Weiler \& Cesa(2021)Weiler and
  Cesa]{weiler2021generale2equivariantsteerablecnns}
Maurice Weiler and Gabriele Cesa.
\newblock General $e(2)$-equivariant steerable cnns, 2021.
\newblock URL \url{https://arxiv.org/abs/1911.08251}.

\bibitem[Wu et~al.(2024)Wu, Arora, Wang, Geiger, Jurafsky, Manning, and
  Potts]{wu2024reftrepresentationfinetuninglanguage}
Zhengxuan Wu, Aryaman Arora, Zheng Wang, Atticus Geiger, Dan Jurafsky,
  Christopher~D. Manning, and Christopher Potts.
\newblock Reft: Representation finetuning for language models, 2024.
\newblock URL \url{https://arxiv.org/abs/2404.03592}.

\bibitem[Wurgaft et~al.(2025)Wurgaft, Lubana, Park, Tanaka, Reddy, and
  Goodman]{wurgaft2025incontextlearningstrategiesemerge}
Daniel Wurgaft, Ekdeep~Singh Lubana, Core~Francisco Park, Hidenori Tanaka,
  Gautam Reddy, and Noah~D. Goodman.
\newblock In-context learning strategies emerge rationally, 2025.
\newblock URL \url{https://arxiv.org/abs/2506.17859}.

\bibitem[Xie et~al.(2021)Xie, Raghunathan, Liang, and Ma]{xie2021explanation}
Sang~Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma.
\newblock An explanation of in-context learning as implicit bayesian inference.
\newblock \emph{arXiv preprint arXiv:2111.02080}, 2021.

\bibitem[Yang et~al.(2024)Yang, Yang, Zhang, Hui, Zheng, Yu, Li, Liu, Huang,
  Wei, et~al.]{yang2024qwen2}
An~Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo~Zheng, Bowen Yu,
  Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et~al.
\newblock Qwen2. 5 technical report.
\newblock \emph{arXiv preprint arXiv:2412.15115}, 2024.

\bibitem[Yang et~al.(2025)Yang, Kautz, and
  Hatamizadeh]{yang2025gateddeltanetworksimproving}
Songlin Yang, Jan Kautz, and Ali Hatamizadeh.
\newblock Gated delta networks: Improving mamba2 with delta rule, 2025.
\newblock URL \url{https://arxiv.org/abs/2412.06464}.

\bibitem[Yue et~al.(2025)Yue, Chen, Lu, Zhao, Wang, Yue, Song, and
  Huang]{yue2025doesreinforcementlearningreally}
Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song,
  and Gao Huang.
\newblock Does reinforcement learning really incentivize reasoning capacity in
  llms beyond the base model?, 2025.
\newblock URL \url{https://arxiv.org/abs/2504.13837}.

\bibitem[Zhang et~al.(2025)Zhang, Patel, Rizvi, Liu, He, Karbasi, Zappala, and
  van Dijk]{zhang2025intelligenceedgechaos}
Shiyang Zhang, Aakash Patel, Syed~A Rizvi, Nianchen Liu, Sizhuang He, Amin
  Karbasi, Emanuele Zappala, and David van Dijk.
\newblock Intelligence at the edge of chaos, 2025.
\newblock URL \url{https://arxiv.org/abs/2410.02536}.

\bibitem[Zhao et~al.(2025)Zhao, Meterez, Kakade, Pehlevan, Jelassi, and
  Malach]{zhao2025echochamberrlposttraining}
Rosie Zhao, Alexandru Meterez, Sham Kakade, Cengiz Pehlevan, Samy Jelassi, and
  Eran Malach.
\newblock Echo chamber: Rl post-training amplifies behaviors learned in
  pretraining, 2025.
\newblock URL \url{https://arxiv.org/abs/2504.07912}.

\bibitem[Zweiger et~al.(2025)Zweiger, Pari, Guo, Akyürek, Kim, and
  Agrawal]{zweiger2025selfadaptinglanguagemodels}
Adam Zweiger, Jyothish Pari, Han Guo, Ekin Akyürek, Yoon Kim, and Pulkit
  Agrawal.
\newblock Self-adapting language models, 2025.
\newblock URL \url{https://arxiv.org/abs/2506.10943}.

\end{thebibliography}
